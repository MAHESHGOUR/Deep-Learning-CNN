{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data using Data Genrator"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nprint(\"TensorFlow version is \", tf.__version__)\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport cv2\nfrom sklearn.metrics import classification_report\nimport numpy as np\nfrom sklearn.utils import shuffle\nfrom keras import regularizers\nfrom keras.models import Sequential,Model,load_model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalAveragePooling2D\nfrom keras.preprocessing.image import ImageDataGenerator\nimport keras.layers as Layers\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport keras.optimizers as Optimizer\nprint(tf.__version__)\nfrom keras import applications\nfrom tensorflow import keras\n\n# stacked generalization with linear meta model on blobs dataset\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom keras.models import load_model\nfrom keras.utils import to_categorical\nfrom numpy import dstack","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use_PreTrain_model()"},{"metadata":{"trusted":true},"cell_type":"code","source":"def use_PreTrain_model(): \n    import tensorflow as tf\n    img_height,img_width = 224,224 \n    IMG_SHAPE = (img_height,img_width, 3)\n    inp = tf.keras.Input(shape=(img_height,img_width, 3))\n    inp2 = tf.keras.layers.Concatenate()([inp, inp, inp])\n\n    #base_model = tf.compat.v2.keras.applications.ResNet152(include_top=False, weights='imagenet', input_tensor=None, input_shape=IMG_SHAPE, pooling=None, classes=2)\n\n    #base_model = tf.compat.v2.keras.applications.ResNet152(include_top=False, weights='imagenet', input_tensor=None, input_shape=IMG_SHAPE, pooling=None, classes=2)\n\n    #base_model = tf.compat.v2.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=IMG_SHAPE, pooling=None, classes=3)\n    #base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet', input_tensor=None, input_shape=IMG_SHAPE, pooling=None, classes=3)\n    #base_model =  tf.compat.v2.keras.applications.Xception(include_top=False, weights='imagenet', input_tensor=None, input_shape=IMG_SHAPE, pooling=None, classes=1000)\n    #base_model = tf.compat.v2.keras.applications.VGG19(include_top=False, weights='imagenet', input_tensor=None, input_shape=IMG_SHAPE, pooling=None, classes=1000)\n    #base_model = tf.keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=IMG_SHAPE, pooling=None, classes=1000)\n    base_model =tf.keras.applications.DenseNet121(include_top=False, weights='imagenet', input_tensor=None, input_shape=IMG_SHAPE, pooling=None, classes=1000)\n    #base_model =tf.keras.applications.DenseNet169(include_top=False, weights='imagenet', input_tensor=None, input_shape=IMG_SHAPE, pooling=None, classes=1000)\n    #base_model = tf.keras.applications.MobileNetV2(include_top=False, weights='imagenet', input_tensor=None, input_shape=IMG_SHAPE, pooling=None, classes=1000)\n    #base_model =tf.keras.applications.NASNetLarge(include_top=False, weights='imagenet', input_tensor=None, input_shape=IMG_SHAPE, pooling=None, classes=1000)\n  \n    \n    \n    base_model.trainable = True\n    # Let's take a look to see how many layers are in the base model\n    print(\"Number of layers in the base model: \", len(base_model.layers))\n    # Fine-tune from this layer onwards\n    fine_tune_at =422\n\n    # Freeze all the layers before the `fine_tune_at` layer\n    for layer in base_model.layers[:fine_tune_at]:\n      layer.trainable =  False\n         \n    model = tf.keras.Sequential([\n          base_model,\n          keras.layers.Conv2D(512, (3, 3), activation='relu'),\n          keras.layers.Conv2D(512, (3, 3), activation='relu'),\n\n          keras.layers.GlobalAveragePooling2D(),\n          #MCDropout(rate=0.5),\n          keras.layers.Dense(3, activation='softmax')\n    ])\n    return model \n#model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dataGen(batch, path) :\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    # All images will be rescaled by 1./255\n    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=15,  \n                                       width_shift_range=0.4,height_shift_range=0.3, horizontal_flip=True,\n                                       shear_range=0.2, zoom_range=0.2)\n    # fit the data augmentation\n    #train_datagen.fit(x_train)\n      \n    valid_datagen = ImageDataGenerator(rescale=1./255)#,featurewise_center=True, \n                                       #featurewise_std_normalization=True, rotation_range=20,   \n                                       #width_shift_range=0.2,height_shift_range=0.2, horizontal_flip=True)\n\n    test_datagen = ImageDataGenerator(rescale=1./255)\n    size=(224,224)\n        \n    train_generator = train_datagen.flow_from_directory(\n            path+'Train',  # This is the source directory for training images\n            target_size=size,  # All images will be resized to 150x150\n            batch_size=batch,\n            # Since we use binary_crossentropy loss, we need binary labels\n            class_mode='categorical')\n    label_map = (train_generator.class_indices)\n\n    test_generator = test_datagen.flow_from_directory(\n            path+'test/',  # This is the source directory for training images\n            target_size=size,  # All images will be resized to 150x150\n            batch_size=batch,\n            # Since we use binary_crossentropy loss, we need binary labels\n            class_mode='categorical',\n            shuffle=False)\n\n    valid_generator = valid_datagen.flow_from_directory(\n            path+'validation/',  # This is the source directory for training images\n            target_size=size,  # All images will be resized to 150x150\n            batch_size=batch,\n            # Since we use binary_crossentropy loss, we need binary labels\n            class_mode='categorical')\n    return train_generator, test_generator, valid_generator\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from keras import optimizers\n#model = get_model(mc=True)\nmodel = use_PreTrain_model()\npath='../input/covid19cxr2-dataset/fold4/fold4/'\n\ntrain_generator, test_generator, valid_generator = dataGen(16,path)\n#model=use_PreTrain_model()\nn_epochs =10\n\nmodel.compile(tf.keras.optimizers.SGD(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])# define learning rate callback\n#model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001, decay=1e-6),\n#              loss='binary_crossentropy',metrics=['Accuracy'])# define learning rate callback\n\ncheckpointer = ModelCheckpoint('mob_model-1.h5',monitor='val_loss', verbose=1, save_best_only=True)\n\nhistory = model.fit_generator(train_generator, epochs=n_epochs, verbose= 1, shuffle = True, \n                              validation_data = valid_generator,steps_per_epoch=40,\n                              validation_steps=2, callbacks=[checkpointer])\n\nmodel.save('mob_model.h5')\n\n# evaluate the model\n_, train_acc = model.evaluate(train_generator, verbose=0)\n_, test_acc = model.evaluate(test_generator, verbose=0)\n\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\nmodel1 = load_model('mob_model-1.h5')\n_, test_acc = model1.evaluate(test_generator, verbose=0)\nprint('Train by mob_model-1: %.3f' % (test_acc))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"fold1-608,  fold2-,   fold3-,  fold4-,  fold5-"},{"metadata":{"trusted":true},"cell_type":"code","source":"path='../input/covid19cxr-dataset2/fold5/fold5/'\ntrain_generator, test_generator, valid_generator = dataGen(16,path) \ntest_generator.reset()\n#test_Images, test_labels = test_generator.next()\n#train_generator, test_generator, valid_generator = dataGen(304,path)\n#validation_Images, validation_labels = valid_generator.next() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom imblearn.metrics import sensitivity_specificity_support\nfrom sklearn.metrics import classification_report\nfrom imblearn.metrics import geometric_mean_score\nimport seaborn as sn\nimport pandas as pd\nmodel = load_model('../input/stack-cnn-submodels/vgg19/vgg19/vgg_F5_model_9.h5')\ntest_generator.reset()\npred = model.predict_generator(test_generator, verbose=0, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n#pred = model.predict_generator(test_generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n#pred_label, true_label = test_Images, test_labels\n# evaluate standalone models on test dataset\npred_label=np.argmax(pred,axis=1)\ntrue_label = test_generator.classes\n\nloss, acc = model.evaluate_generator(test_generator,steps=None)\n\ntarget_names = ['COVID19',  'Normal','Pneumonia']\nresult = sensitivity_specificity_support(true_label, pred_label, average='macro')\n\nprint(\"Sensitivity: {:5.2f}%\".format(100*result[0]), \"specificity {:5.2f}%\".format(100*result[1]), \n      \"Accuracy: {:5.2f}%\".format(100*acc),'\\n')\n\nreport=classification_report(true_label, pred_label, target_names=target_names, digits=4)\nprint(report)\n\nf = open( 'report.txt', 'w' )\nf.write(report)\nf.close()\n\n\ndisp = confusion_matrix(true_label, pred_label)\ndisp.astype('int')\npd.options.display.float_format = '{:.5f}'.format\ndf_cm = pd.DataFrame(disp, target_names, target_names)\n\nfig, ax = plt.subplots(figsize=(4,3))\nsn.set(font_scale=1.2) # for label size\nsn.heatmap(df_cm, annot=True, annot_kws={\"size\": 13},ax=ax, cmap=\"YlGnBu\" , fmt='g',cbar=False)#, fontsize =12)\nplt.ylabel('Actual',fontsize=16,fontweight='bold')\nplt.xlabel('Predicted',fontsize=16,fontweight='bold')\nplt.ioff()\nimport pylab\npylab.savefig('fold_conf_mat.eps', format = 'eps',bbox_inches='tight')\n\n##### Sensivity and PPV ------------------------------------------------------------------------------------\nmatrix = confusion_matrix(true_label, pred_label)\nmatrix = matrix.astype('float')\n#cm_norm = matrix / matrix.sum(axis=1)[:, np.newaxis]\n#print(matrix)\n#class_acc = np.array(cm_norm.diagonal())\n\nGmean = geometric_mean_score(true_label, pred_label, average='multiclass')\nprint('Gmean: {:.4f}'.format(Gmean))\n\nclass_acc = [matrix[i,i]/np.sum(matrix[i,:]) if np.sum(matrix[i,:]) else 0 for i in range(len(matrix))]\nprint('Sens COVID-19: {0:.3f}, Normal: {1:.3f}, Pneumonia: {2:.3f}'.format(class_acc[0], class_acc[1], class_acc[2]))\n                                                                             \nppvs = [matrix[i,i]/np.sum(matrix[:,i]) if np.sum(matrix[:,i]) else 0 for i in range(len(matrix))]\nprint('PPV COVID-19: {0:.3f}, Normal: {1:.3f}, Pneumonia: {2:.3f}'.format(ppvs[0],  ppvs[1], ppvs[2]))\n\n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy import interp\nfrom sklearn.metrics import roc_auc_score\nfrom keras.utils import to_categorical\n\ntrue_label2 = to_categorical(true_label)\n#pred_label = yscore\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nyscore=pred\nroc_auc = dict()\nn_classes = 3\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(true_label2[:, i], yscore[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(true_label2.ravel(), yscore.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# First aggregate all false positive rates\nlw = 2\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr /= n_classes\n\n#fpr[\"macro\"] = all_fpr\n#tpr[\"macro\"] = mean_tpr\n#roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='Micro-Avg(AUC={0:0.3f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='k', linestyle=':', linewidth=1.2)\n\n\n\nplt.plot(fpr[0], tpr[0], color='b',\n         lw=lw, label='COVID-19(AUC=%0.3f)' % roc_auc[0],linewidth=1.1)\n\nplt.plot(fpr[1], tpr[1], color='g',\n         lw=lw, label='Normal(AUC=%0.3f)' % roc_auc[1],linewidth=1.2)\n\nplt.plot(fpr[2], tpr[2], color='r',\n         lw=lw, label='Pneumonia(AUC=%0.3f)' % roc_auc[2],linewidth=1)\n\n\nimport matplotlib \n\nsize=12\nparams = {'legend.fontsize': 8.5,\n          'figure.figsize': (3,2),\n          'axes.labelsize': size,\n          'axes.titlesize': size,\n          'xtick.labelsize': size*0.8,\n          'ytick.labelsize': size*0.8,\n          'lines.linewidth':1,\n          'axes.titlepad': 6}\n\nplt.rcParams.update(params)\n\n#plt.style.use('classic')\nplt.rc('lines', linewidth=4)\n#plt.legend.fontsize = 10\n#plt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([-0.007, 1.0])\nplt.ylim(top=1.003)\nplt.ylim(bottom=.70)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n#plt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n#plt.rcParams[\"figure.figsize\"] = (6,4)\npylab.savefig('foldROC.eps', format = 'eps',bbox_inches='tight')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Grad-CAM\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import *\nfrom keras.callbacks import *\nimport keras.backend as K\n#from model import *\n#from data import *\nimport cv2\nimport argparse\n# import Keras's functional api\nfrom keras.models import Model\n\nmodel = load_model('../input/models-covid19xray2/fold1/EfN_model.h5')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Load an color image in grayscale\nimg1 = cv2.imread(\"../input/covid19cxr/fold1/fold1/Train/Train/COVID19/COVID-00058.jpg\")\nimg1 =cv2.resize(img1, (224, 224))\nimg2 = cv2.imread(\"../input/covid19cxr/fold1/fold1/Train/Train/COVID19/COVID-00060-b.jpg\")\nimg2 =cv2.resize(img2, (224, 224))\nimg3 = cv2.imread(\"../input/covid19cxr/fold1/fold1/Train/Train/COVID19/COVID-00077.jpg\")\nimg3 =cv2.resize(img3, (224, 224))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_path =\"../input/covid19cxr/fold1/fold1/Train/Train/PNEUMONIA/PNEUMONIA_person14_virus_44.jpeg\"\noriginal_img = cv2.imread(img_path, 1)\noriginal_img =cv2.resize(original_img, (224, 224))\nwidth, height = 224, 224\n\noriginal_img  = original_img/255\noriginal_img  = original_img.astype('float')\noriginal_img.shape\n\n#Reshape to the network input shape (3, w, h).\nimg = np.array([np.transpose(np.float32(original_img), (0, 1, 2))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_img(img):\n    img  = np.array(img,dtype='float')\n    #img = img.reshape((224,224))\n     \n    plt.imshow(img)\n#x = np.random.randint(0,X_test.shape[0])\nshow_img(original_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the 512 input weights to the softmax.\nclass_weights = model.layers[-1].get_weights()[0]\n#final_conv_layer = \"swish_78\"\nget_output = K.function([model.layers[0].input], [model.get_layer(\"conv2d_104\").output, model.layers[-1].output])\n[conv_outputs, predictions] = get_output([img])\nconv_outputs = conv_outputs[0, :, :, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create the class activation map.\ncam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[1:3])\nfor i, w in enumerate(class_weights[1 ,:]): \n    cam += w * conv_outputs[i, :, :]\nprint(\"predictions\", predictions)\ncam /= np.max(cam)\ncam = cv2.resize(cam, (height, width))\nheatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\nheatmap[np.where(cam < 0.2)] = 0\nimg = heatmap*0.5 + original_img\n#cv2.imwrite('../input/output/',img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_img(img):\n    img  = np.array(img,dtype='float')\n    #img = img.reshape((224,224))\n     \n    plt.imshow(img)\n#x = np.random.randint(0,X_test.shape[0])\nshow_img(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nimshow(heatmap.astype(np.uint8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv2.imshow('img',img) \n#np.shape(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_class_activation_map(model_path, img_path, output_path):\n        model = load_model(model_path)\n        original_img = cv2.imread(img_path, 1)\n        width, height, _ = original_img.shape\n\n        #Reshape to the network input shape (3, w, h).\n        img = np.array([np.transpose(np.float32(original_img), (2, 0, 1))])\n        \n        #Get the 512 input weights to the softmax.\n        class_weights = model.layers[-1].get_weights()[0]\n        final_conv_layer = get_output_layer(model, \"conv5_3\")\n        get_output = K.function([model.layers[0].input], [final_conv_layer.output, model.layers[-1].output])\n        [conv_outputs, predictions] = get_output([img])\n        conv_outputs = conv_outputs[0, :, :, :]\n\n        #Create the class activation map.\n        cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[1:3])\n        for i, w in enumerate(class_weights[:, 1]):\n                cam += w * conv_outputs[i, :, :]\n        print \"predictions\", predictions\n        cam /= np.max(cam)\n        cam = cv2.resize(cam, (height, width))\n        heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n        heatmap[np.where(cam < 0.2)] = 0\n        img = heatmap*0.5 + original_img\n        cv2.imwrite(output_path, img)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train = np.zeros((1, 224, 224, 3),dtype=np.uint8)\nY_train[0] = img1\nY_train[1] = img2\nY_train[2] = img3\n                   \nnp.shape(Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom keras.applications.vgg16 import decode_predictions\npreds = model.predict(Y_train)\npredictions = pd.DataFrame(decode_predictions(preds, top=3)[0],columns=['col1','category','probability']).iloc[:,1:]\nprint('PREDICTION:',predictions.loc[0,'category'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"argmax = np.argmax(preds[0])\noutput = model.output[:, argmax]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model._layers_by_depth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_conv_layer = model.get_layer('conv2d_104')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grads = K.gradients(output, last_conv_layer.output)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pooled_grads = K.mean(grads, axis=(0, 1, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_img_size_h,train_img_size_w =224,224","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the prediction for 10 test images\nfor idx in range(3):   \n    # get the feature map of the test image\n    features_for_one_img = features[idx, :, :, :]\n\n    # map the feature map to the original size\n    height_roomout = train_img_size_h / features_for_one_img.shape[0]\n    width_roomout = train_img_size_w / features_for_one_img.shape[1]\n    cam_features = sp.ndimage.zoom(features_for_one_img, (height_roomout, width_roomout, 1), order=2)\n        \n    # get the predicted label with the maximum probability\n    pred = np.argmax(results[idx])\n    \n    # prepare the final display\n    plt.figure(facecolor='white')\n    \n    # get the weights of class activation map\n    cam_weights = gap_weights[:, pred]\n\n    # create the class activation map\n    cam_output = np.dot(cam_features, cam_weights)\n    \n    # draw the class activation map\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    \n    buf = 'Predicted Class = ' + fashion_name[pred] + ', Probability = ' + str(results[idx][pred])\n    plt.xlabel(buf)\n    plt.imshow(t_pic[idx], alpha=0.5)\n    plt.imshow(cam_output, cmap='jet', alpha=0.5)\n     \n    plt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get normalized input. VGG network handles the normalized image internally. \n#batch1_img = img1.reshape((1, 224, 224, 3))\nbatch1_label = np.array([1 if i == 242 else 0 for i in range(1000)])  # 1-hot result for Boxer\nbatch1_label = batch1_label.reshape(1, -1)\n\n#batch2_img = img2.reshape((1, 224, 224, 3))\nbatch2_label = np.array([1 if i == 155 else 0 for i in range(1000)])  # 1-hot result for Shih-Tzu\nbatch2_label = batch2_label.reshape(1, -1)\n\n#batch3_img = img3.reshape((1, 224, 224, 3))\nbatch3_label = np.array([1 if i == 292 else 0 for i in range(1000)])  # 1-hot result for tiger\nbatch3_label = batch3_label.reshape(1, -1)\n\nbatch_img = np.concatenate((img1, img2, img3), 0)\nbatch_label = np.concatenate((batch1_label, batch2_label, batch3_label), 0)\n\nbatch_size = 3\n\n# for i in range(batch_size):\n#     print('See visualization of below category')\n#     utils.print_prob(batch_label[i], './synset.txt')\n\n# Create tensorflow graph for evaluation\neval_graph = tf.Graph()\nwith eval_graph.as_default():\n    with eval_graph.gradient_override_map({'Relu': 'GuidedRelu'}):\n    \n        images = tf.placeholder(\"float\", [batch_size, 224, 224, 3])\n        labels = tf.placeholder(tf.float32, [batch_size, 1000])\n\n        vgg = vgg16.Vgg16()\n        \n        vgg.build(images)\n        cost = (-1) * tf.reduce_sum(tf.multiply(labels, tf.log(vgg.prob)), axis=1)\n        print('cost:', cost)\n        # cost = tf.reduce_sum((vgg.prob - labels) ** 2)\n        \n        \n        # gradient for partial linearization. We only care about target visualization class. \n        y_c = tf.reduce_sum(tf.multiply(vgg.fc8, labels), axis=1)\n        print('y_c:', y_c)\n        # Get last convolutional layer gradient for generating gradCAM visualization\n        target_conv_layer = vgg.pool5\n        target_conv_layer_grad = tf.gradients(y_c, target_conv_layer)[0]\n\n        # Guided backpropagtion back to input layer\n        gb_grad = tf.gradients(cost, images)[0]\n\n        init = tf.global_variables_initializer()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run tensorflow \n\nwith tf.Session(graph=eval_graph) as sess:    \n    sess.run(init)\n    \n    prob = sess.run(vgg.prob, feed_dict={images: batch_img})\n    \n    gb_grad_value, target_conv_layer_value, target_conv_layer_grad_value = sess.run([gb_grad, target_conv_layer, target_conv_layer_grad], feed_dict={images: batch_img, labels: batch_label})\n    \n    \n    for i in range(batch_size):\n        utils.print_prob(prob[i], './synset.txt')\n        # VGG16 use BGR internally, so we manually change BGR to RGB\n        gradBGR = gb_grad_value[i]\n        gradRGB = np.dstack((\n            gradBGR[:, :, 2],\n            gradBGR[:, :, 1],\n            gradBGR[:, :, 0],\n        ))\n        utils.visualize(batch_img[i], target_conv_layer_value[i], target_conv_layer_grad_value[i], gradRGB)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nnames = []\n\nsensitivity = [94.03, 91.45, 92.54, 91.78, 94.17] \n\nresults.append(sensitivity)\nnames.append('Sensitivity')\nspecificity = (96.99, 94.47, 95.15, 94.42, 95.41 ) \nresults.append(specificity)\nnames.append('Specificity')\naccuracy = (94.95, 90.53, 91.52, 89.86, 92.01) \nresults.append(accuracy)\nnames.append('Accuracy')\nPPV = (96.23, 89.83, 92.30, 86.20, 91.70)\nresults.append(PPV)\nnames.append('PPV')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas\nimport matplotlib.pyplot as plt\nimport statistics \n# boxplot algorithm comparison\nresults = []\nnames = []\n\n#sensitivity = [94.03, 91.45, 92.54, 91.78, 94.17] \nwith open(\"../input/models-covid19xray2/fold1/acc.txt\", \"rb\") as fp:   # Unpickling\n    accs = pickle.load(fp)\nresults.append(accs)\nnames.append('fold1')\nwith open(\"../input/models-covid19xray2/fold2/acc.txt\", \"rb\") as fp:   # Unpickling\n    accs = pickle.load(fp)\nresults.append(accs)\nnames.append('fold2')\nwith open(\"../input/models-covid19xray2/fold3/acc.txt\", \"rb\") as fp:   # Unpickling\n    accs = pickle.load(fp)\nresults.append(accs)\nnames.append('fold3')\nwith open(\"../input/models-covid19xray2/fold4/acc.txt\", \"rb\") as fp:   # Unpickling\n    accs = pickle.load(fp)\nresults.append(accs)\nnames.append('fold4')\nwith open(\"../input/models-covid19xray2/fold5/acc.txt\", \"rb\") as fp:   # Unpickling\n    accs = pickle.load(fp)\nresults.append(accs)\nnames.append('fold5')\n#print(\"Standard Deviation of sensi {:.4f} \".format(statistics.stdev(accs))) \n#print(\"Variance of sample set is {:.5f}\".format(statistics.variance(accs))) \n\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results, showfliers=True, widths =0.4)\nax.set_xticklabels(names)\nplt.rcParams[\"figure.figsize\"] = (8,4)\nplt.savefig('boxplot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Make some fake data.\na = b = np.arange(0, 3, .02)\nc = np.exp(a)\nd = c[::-1]\n\n# Create plots with pre-defined labels.\nfig, ax = plt.subplots()\nax.plot(a, c, 'k--', label='Model length')\nax.plot(a, d, 'k:', label='Data length')\nax.plot(a, c + d, 'k', label='Total message length')\n\nlegend = ax.legend(loc='upper center', shadow=True, fontsize='x-large')\n\n# Put a nicer background color on the legend.\nlegend.get_frame().set_facecolor('#00FFCC')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = 247\nplt.imshow(test_Images[idx][:,:,0])\np0 = np.array([p[idx] for p in mc_predictions])\nprint(\"posterior mean: {}\".format(p0.mean(axis=0).argmax()))\nprint(\"true label: {}\".format(test_labels[idx].argmax()))\nprint()\n# probability + variance\nfor i, (prob, var) in enumerate(zip(p0.mean(axis=0), p0.std(axis=0))):\n    print(\"class: {}; proba: {:.1%}; var: {:.2%} \".format(i, prob, var))\n\nx, y = list(range(len(p0.mean(axis=0)))), p0.mean(axis=0)\nplt.plot(x, y);\nfig, axes = plt.subplots(3, 1, figsize=(12,12))\n\nfor i, ax in enumerate(fig.get_axes()):\n    ax.hist(p0[:,i], bins=100, range=(0,1));\n    ax.set_title(f\"class {i}\")\n    ax.label_outer()\n    \nm = tf.keras.metrics.Accuracy()\nm.update_state(true_label, pred_label)\nm.result().numpy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nStandard Normal Distribution\nAuthor: Balamurali M\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass norm1:\n    def __init__(self, a1, b1, c1):\n        self.a1 = a1\n        self.b1 = b1\n        self.c1 = c1\n        \n    def dist_curve(self):\n        plt.plot(self.c1, 1/(self.b1 * np.sqrt(2 * np.pi)) *\n            np.exp( - (self.c1 - self.a1)**2 / (2 * self.b1**2) ), linewidth=2, color='y')\n        plt.show()\n\n#mean 0 and sd 1 for the standard normal distribution\n\n\nmean, sd = sample_mean,pop_stdev\n\n\nc = sample#np.random.normal(mean, sd, 3000)\n        \nw1, x1, z1 = plt.hist(c, 10, density=True,stacked=True,cumulative=False) #hist\n\nhist1 = norm1(mean, sd, x1)\nplot1 = hist1.dist_curve()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}