{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "covid19detection1-pretrain.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXR5C97Qn4H9"
      },
      "source": [
        "# Load data using Data Genrator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnGYsoOun4H9",
        "outputId": "a396b5d6-ef15-4f33-e9bc-7465a18bb883"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print(\"TensorFlow version is \", tf.__version__)\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import cv2\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential,Model,load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalAveragePooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras.layers as Layers\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import keras.optimizers as Optimizer\n",
        "print(tf.__version__)\n",
        "from keras import applications\n",
        "from tensorflow import keras\n",
        "\n",
        "# stacked generalization with linear meta model on blobs dataset\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "from numpy import dstack"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version is  2.4.1\n",
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy6vQXSGv86_",
        "outputId": "a8f46787-288c-4bbe-edca-384131e372f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY4Igz_Wn4H-"
      },
      "source": [
        "# Use_PreTrain_model()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5FKrQK6w6wT"
      },
      "source": [
        "# https://pypi.python.org/pypi/libarchive\n",
        "!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n",
        "import libarchive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "otWdTuDSn4H_"
      },
      "source": [
        "def use_PreTrain_model(): \n",
        "    import tensorflow as tf\n",
        "    img_height,img_width = 224,224 \n",
        "    IMG_SHAPE = (img_height,img_width, 3)\n",
        "    inp = tf.keras.Input(shape=(img_height,img_width, 3))\n",
        "    inp2 = tf.keras.layers.Concatenate()([inp, inp, inp])\n",
        "\n",
        "    \n",
        "    base_model = tf.compat.v2.keras.applications.VGG19(include_top=False, weights='imagenet', input_tensor=None, input_shape=IMG_SHAPE, pooling=None, classes=1000)\n",
        "    #base_model = tf.keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=IMG_SHAPE, pooling=None, classes=1000)\n",
        "     \n",
        "        \n",
        "    base_model.trainable = True\n",
        "    # Let's take a look to see how many layers are in the base model\n",
        "    print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "    # Fine-tune from this layer onwards\n",
        "    fine_tune_at =18\n",
        "\n",
        "    # Freeze all the layers before the `fine_tune_at` layer\n",
        "    for layer in base_model.layers[:fine_tune_at]:\n",
        "      layer.trainable =  False\n",
        "         \n",
        "    model = tf.keras.Sequential([\n",
        "          base_model,\n",
        "          keras.layers.Conv2D(512, (3, 3), activation='relu'),\n",
        "          keras.layers.Conv2D(512, (3, 3), activation='relu'),\n",
        "\n",
        "          keras.layers.GlobalAveragePooling2D(),\n",
        "          #MCDropout(rate=0.5),\n",
        "          keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "    return model \n",
        "#model.summary()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlz7JFd-xkZg"
      },
      "source": [
        "# Load COVID-19 X-ray images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ff8nEWKIn4IA"
      },
      "source": [
        "def dataGen(batch, path) :\n",
        "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "    # All images will be rescaled by 1./255\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=15,  \n",
        "                                       width_shift_range=0.4,height_shift_range=0.3, horizontal_flip=True,\n",
        "                                       shear_range=0.2, zoom_range=0.2)\n",
        "    # fit the data augmentation\n",
        "    #train_datagen.fit(x_train)\n",
        "      \n",
        "    valid_datagen = ImageDataGenerator(rescale=1./255)#,featurewise_center=True, \n",
        "                                       #featurewise_std_normalization=True, rotation_range=20,   \n",
        "                                       #width_shift_range=0.2,height_shift_range=0.2, horizontal_flip=True)\n",
        "\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    size=(224,224)\n",
        "        \n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "            path+'Train',  # This is the source directory for training images\n",
        "            target_size=size,  # All images will be resized to 150x150\n",
        "            batch_size=batch,\n",
        "            # Since we use binary_crossentropy loss, we need binary labels\n",
        "            class_mode='categorical')\n",
        "    label_map = (train_generator.class_indices)\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "            path+'test/',  # This is the source directory for training images\n",
        "            target_size=size,  # All images will be resized to 150x150\n",
        "            batch_size=batch,\n",
        "            # Since we use binary_crossentropy loss, we need binary labels\n",
        "            class_mode='categorical',\n",
        "            shuffle=False)\n",
        "\n",
        "    valid_generator = valid_datagen.flow_from_directory(\n",
        "            path+'validation/',  # This is the source directory for training images\n",
        "            target_size=size,  # All images will be resized to 150x150\n",
        "            batch_size=batch,\n",
        "            # Since we use binary_crossentropy loss, we need binary labels\n",
        "            class_mode='categorical')\n",
        "    return train_generator, test_generator, valid_generator\n",
        "\n",
        "\n",
        "path='../input/covid19cxr2-dataset/fold4/fold4/'\n",
        "train_generator, test_generator, valid_generator = dataGen(16,path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-DLNQtPxxOM"
      },
      "source": [
        "# Transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_nI9T2Uen4IB"
      },
      "source": [
        "\n",
        "model = use_PreTrain_model()\n",
        "n_epochs =10\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001, decay=1e-6),\n",
        "              loss='binary_crossentropy',metrics=['Accuracy'])# define learning rate callback\n",
        "\n",
        "\n",
        "history = model.fit_generator(train_generator, epochs=n_epochs, verbose= 1, shuffle = True, \n",
        "                              validation_data = valid_generator,steps_per_epoch=40,\n",
        "                              validation_steps=2)\n",
        "\n",
        "model.save('mob_model.h5')\n",
        "\n",
        "\n",
        "# evaluate the model\n",
        "\n",
        "_, train_acc = model.evaluate(train_generator, verbose=0)\n",
        "_, test_acc = model.evaluate(test_generator, verbose=0)\n",
        "\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "model1 = load_model('mob_model-1.h5')\n",
        "_, test_acc = model1.evaluate(test_generator, verbose=0)\n",
        "print('Train by mob_model-1: %.3f' % (test_acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGXjHm93zLqs"
      },
      "source": [
        "# Performances Assisment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7FYGTM7Vn4ID"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from imblearn.metrics import sensitivity_specificity_support\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "model = load_model('../input/stack-cnn-submodels/vgg19/vgg19/vgg_F5_model_9.h5')\n",
        "test_generator.reset()\n",
        "pred = model.predict_generator(test_generator, verbose=0, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
        "\n",
        "pred_label=np.argmax(pred,axis=1)\n",
        "true_label = test_generator.classes\n",
        "\n",
        "loss, acc = model.evaluate_generator(test_generator,steps=None)\n",
        "\n",
        "target_names = ['COVID19',  'Normal']\n",
        "result = sensitivity_specificity_support(true_label, pred_label, average='macro')\n",
        "\n",
        "print(\"Sensitivity: {:5.2f}%\".format(100*result[0]), \"specificity {:5.2f}%\".format(100*result[1]), \n",
        "      \"Accuracy: {:5.2f}%\".format(100*acc),'\\n')\n",
        "\n",
        "report=classification_report(true_label, pred_label, target_names=target_names, digits=4)\n",
        "print(report)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "C-0gzHuNn4IG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}